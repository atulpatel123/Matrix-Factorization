{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VOCexhHBwZka"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy\n",
        "\n",
        "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
        "    '''\n",
        "    R: rating matrix\n",
        "    P: |U| * K (User features matrix)\n",
        "    Q: |D| * K (Item features matrix)\n",
        "    K: latent features\n",
        "    steps: iterations\n",
        "    alpha: learning rate\n",
        "    beta: regularization parameter'''\n",
        "    Q = Q.T\n",
        "\n",
        "    for step in range(steps):\n",
        "        for i in range(len(R)):\n",
        "            for j in range(len(R[i])):\n",
        "                if R[i][j] > 0:\n",
        "                    # calculate error\n",
        "                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])\n",
        "\n",
        "                    for k in range(K):\n",
        "                        # calculate gradient with a and beta parameter\n",
        "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
        "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
        "\n",
        "        eR = numpy.dot(P,Q)\n",
        "\n",
        "        e = 0\n",
        "\n",
        "        for i in range(len(R)):\n",
        "\n",
        "            for j in range(len(R[i])):\n",
        "\n",
        "                if R[i][j] > 0:\n",
        "\n",
        "                    e = e + pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2)\n",
        "\n",
        "                    for k in range(K):\n",
        "\n",
        "                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
        "        # 0.001: local minimum\n",
        "        if e < 0.001:\n",
        "\n",
        "            break\n",
        "\n",
        "    return P, Q.T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R = [\n",
        "\n",
        "     [5,3,0,1],\n",
        "\n",
        "     [4,0,0,1],\n",
        "\n",
        "     [1,1,0,5],\n",
        "\n",
        "     [1,0,0,4],\n",
        "\n",
        "     [0,1,5,4],\n",
        "\n",
        "     [2,1,3,0],\n",
        "\n",
        "    ]\n",
        "\n",
        "R = numpy.array(R)\n",
        "# N: num of User\n",
        "N = len(R)\n",
        "# M: num of Movie\n",
        "M = len(R[0])\n",
        "# Num of Features\n",
        "K = 3\n",
        "\n",
        "\n",
        "P = numpy.random.rand(N,K)\n",
        "Q = numpy.random.rand(M,K)\n",
        "\n",
        "\n",
        "\n",
        "nP, nQ = matrix_factorization(R, P, Q, K)\n",
        "\n",
        "nR = numpy.dot(nP, nQ.T)\n",
        "print(nR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsYRcCDxwgZ8",
        "outputId": "1b20030b-bcff-47bf-f0a6-0e6b4bfd4259"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.9893249  2.91266176 5.78037659 1.00917307]\n",
            " [3.96585842 1.27478361 4.34519316 1.00270401]\n",
            " [1.02542119 0.9046394  3.22081417 4.97820202]\n",
            " [0.98532393 1.0684683  2.83330437 3.98268841]\n",
            " [3.39122557 1.07662727 4.97761959 3.97868597]\n",
            " [2.00300403 1.11687437 2.94435072 1.98375958]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSAEEFHLwk8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}